# -*- coding: utf-8 -*-
"""MT_fine-tuning_mbart_en_zh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/191As6_qOCxfd05uL8QUgXEWfSQ2KWhsF

# Fine-Tuning mBART on the United Nations Parallel Corpus (UNPC)

In this project we take the multilingual model mBart and fine-tune it for official document translation using an English-Chinese corpus of UN documents.

Pre-requisites
"""

!pip install --upgrade transformers

pip install --upgrade datasets fsspec

!pip install datasets

!pip install evaluate

!pip install wandb

import transformers
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, DataCollatorForSeq2Seq, TrainingArguments, Trainer
import pandas as pd
from datasets import Dataset, DatasetDict, load_dataset
import json
import re
import torch
from tqdm import tqdm
import evaluate
import wandb
from huggingface_hub import notebook_login

"""# mBart

mBart is a multilingual encoder-decoder (sequence-to-sequence) model primarily intended for translation. It has the same architecture as BART, i.e., 12 encoder layers, 12 decoder layers, and a dimension of 1024.
"""

device="cuda"
model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt").to(device)
tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

print(model)

"""# The United Nations Parallel Corpus v1.0

The UNPC corpus is built from parliamentary documents in the public domain of the United Nations from 1990-2014 and is built to facilitate the multilingual NLP tasks such as statistical machine translation (SMT). The English-Chinese subset of UNPC contains around 17.5 million sentence pairs. The dataset we use is from the HelsinkiNLP group. We are going to use it to fine-tune mBart to perform the task of official document translation.

References:

Michał Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. 2016. The United Nations Parallel Corpus v1.0. In *Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)*, pages 3530–3534, Portorož, Slovenia. European Language Resources Association (ELRA).

https://huggingface.co/datasets/Helsinki-NLP/un_pc

Importing the UNPC (en-zh) Dataset
"""

dataset = load_dataset("Helsinki-NLP/un_pc", "en-zh")
dataset

dataframe = pd.DataFrame(dataset['train'][0:100000]['translation'])
dataframe

ds = Dataset.from_pandas(dataframe)
ds

"""The dataset is splited into the training, validation, and testing dataset, with a proportion of 18:1:1."""

train_test = ds.train_test_split(test_size = 5000 / dataframe.shape[0],seed = 99)

dataset_test = train_test["test"]

target_encodings = tokenizer(train_test["train"]["zh"], max_length = 1024, truncation = True)
input_encodings = tokenizer(train_test["train"]["en"], max_length = 1024, truncation = True)
dataset_pt = Dataset.from_dict(
    {"input_ids": input_encodings["input_ids"],
     "attention_mask": input_encodings["attention_mask"],
     "labels": target_encodings["input_ids"]})
columns = ["input_ids", "labels", "attention_mask"]
dataset_pt.set_format(type = "torch", columns = columns)

train_valid = dataset_pt.train_test_split(test_size = 5000 / (dataframe.shape[0] - 5000), seed = 99)

dataset_train, dataset_valid = train_valid["train"], train_valid["test"]

dataset_train, dataset_valid, dataset_test

"""First of all, we will evaluate the performance of the vanilla mBart on a testing dataset of 5,000 sentence pairs using the BLEU metrics."""

torch.cuda.empty_cache()
model.to("cuda")
tokenizer.src_lang = "en_XX"
target_lang = "zh_CN"
forced_bos_token_id = tokenizer.lang_code_to_id[target_lang]
predictions_vanilla = []
batch_size = 16

for i in tqdm(range(0, len(dataset_test["en"]), batch_size)):
    batch_texts = dataset_test["en"][i:i+batch_size]

    inputs = tokenizer(batch_texts,
                       return_tensors = "pt",
                       padding = "longest",
                       truncation = True,
                       max_length = 256)

    input_ids = inputs["input_ids"].to("cuda")
    attention_mask = inputs["attention_mask"].to("cuda")

    outputs = model.generate(
        input_ids = input_ids,
        attention_mask = attention_mask,
        max_length = 256,
        num_beams =6,
        no_repeat_ngram_size = 2,
        repetition_penalty = 1.2,
        early_stopping = True,
        forced_bos_token_id = forced_bos_token_id
    )

    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    # Post-processing method to remove the spaces
    cleaned_predictions = [re.sub(r'\s+', '', pred.strip()) for pred in decoded]

    predictions_vanilla.extend(cleaned_predictions)

# save the data
with open('predictions_vanilla.json', 'w', encoding = 'utf-8') as out:
    json.dump(predictions_vanilla, out, indent = 2, ensure_ascii = False)

predictions_vanilla
references = [dataset_test["zh"]]
bleu = evaluate.load("bleu")
bleu.add(predictions = str(predictions_vanilla), references = str(references))
bleu.compute()

"""Next we will fine-tune mBart using the training dataset."""

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)

training_args = TrainingArguments(
    output_dir = "en-zh",
    num_train_epochs = 1,
    warmup_steps = 500,
    per_device_train_batch_size = 1,
    per_device_eval_batch_size = 1,
    weight_decay = 0.01,
    logging_steps = 10,
    push_to_hub = False,
    eval_strategy = "steps",
    eval_steps = 30,
    save_steps = 600,
    gradient_accumulation_steps = 128,
    load_best_model_at_end = True,
)

trainer = Trainer(model = model,
                  args = training_args,
                  tokenizer = tokenizer,
                  data_collator = seq2seq_data_collator,
                  train_dataset = dataset_train,
                  eval_dataset = dataset_valid)

notebook_login()
wandb.init(mode = "disabled")

torch.cuda.empty_cache()
trainer.train()
trainer.save_model("mbart-unpc-en-zh")

# load the fine-tuned model after the training is completed
device="cuda"
model = MBartForConditionalGeneration.from_pretrained("./mbart-unpc-en-zh").to(device)
tokenizer = MBart50TokenizerFast.from_pretrained("./mbart-unpc-en-zh")

# or download and load the fine-tuned model mBART-UNPC-en-zh (skip the training process)
!gdown 1Kd25Z7D39xdsaoZOD4XKHDZFnzuuIIT3
!gunzip mbart-UNPC-en-zh.tar.gz
!tar xf mbart-UNPC-en-zh.tar

"""We evaluate the fine-tuned model using BLEU."""

torch.cuda.empty_cache()
model.to("cuda")
tokenizer.src_lang = "en_XX"
target_lang = "zh_CN"
forced_bos_token_id = tokenizer.lang_code_to_id[target_lang]
predictions_finetuned = []
batch_size = 16

for i in tqdm(range(0, len(dataset_test["en"]), batch_size)):
    batch_texts = dataset_test["en"][i:i+batch_size]

    inputs = tokenizer(batch_texts,
                       return_tensors = "pt",
                       padding = "longest",
                       truncation = True,
                       max_length = 256)

    input_ids = inputs["input_ids"].to("cuda")
    attention_mask = inputs["attention_mask"].to("cuda")

    outputs = model.generate(
        input_ids = input_ids,
        attention_mask = attention_mask,
        max_length = 256,
        num_beams =6,
        no_repeat_ngram_size = 2,
        repetition_penalty = 1.2,
        early_stopping = True,
        forced_bos_token_id = forced_bos_token_id
    )

    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    # Post-processing method to remove the spaces
    cleaned_predictions = [re.sub(r'\s+', '', pred.strip()) for pred in decoded]

    predictions_finetuned.extend(cleaned_predictions)

# save the data
with open('predictions_finetuned.json', 'w', encoding = 'utf-8') as out:
    json.dump(predictions_finetuned, out, indent = 2, ensure_ascii = False)

# check the generated translation and compare with the reference translation.
for i in range(100):
    print("Input:", dataset_test["en"][i])
    print("Generated:", predictions_finetuned[i])
    print("Vanilla:", predictions_vanilla[i])
    print("Reference:", dataset_test["zh"][i])
    print("=" * 80)

import evaluate
predictions_finetuned
references = [dataset_test["zh"]]
bleu = evaluate.load("bleu")
bleu.add(predictions = str(predictions_finetuned), references=str(references))
bleu.compute()